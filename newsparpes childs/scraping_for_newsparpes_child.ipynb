{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping DCG.media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SCRAPING\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "### DATABASES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "### VISUALISATION\n",
    "import plotly.express as px\n",
    "\n",
    "### DIVERS\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### FORMAT\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'spotemploi'\n",
    "abbr = 'spe'\n",
    "tld = '.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number = 1\n",
    "top_url = f'https://www.{website+tld}'\n",
    "top_search_url = f'{top_url}/page/{page_number}/?s'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des fonctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir le nombre de pages de résultats de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_page_results(top_search_url):\n",
    "    r = rq.get(top_search_url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    page_numbers = soup.find_all(\"a\", class_=\"page-numbers\")\n",
    "    pages = [int(page.text) for page in page_numbers if page.text.isdigit()]\n",
    "    if pages:\n",
    "        return max(pages)  # Le numéro le plus élevé correspond au total de pages\n",
    "    return 1\n",
    "get_max_page_results(top_search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir la liste des pages de résultats de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.spotemploi.com/page/1/?s']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_results_pages(page_final):\n",
    "    urls = []\n",
    "    for i in range(page_final):\n",
    "        i = f'{top_url}/page/{i+1}/?s'\n",
    "        urls.append(i)\n",
    "    return urls\n",
    "get_all_results_pages(get_max_page_results(top_search_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenir tous les liens des articles sur une page de recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_on_page(search_results):\n",
    "    urls_articles = []\n",
    "    s = rq.Session()\n",
    "    for p in tqdm(search_results):\n",
    "        r = s.get(p)\n",
    "        soup = BeautifulSoup(r.content)\n",
    "        articles = soup.find_all('h2', class_ = 'entry-title')\n",
    "        for article in articles:\n",
    "            url = article.find('a', href=True)['href']\n",
    "            urls_articles.append(url)\n",
    "    return urls_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc35c6f8d8c84284a0a9f87e03783c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['https://www.spotemploi.com/pourquoi-la-formation-est-la-cle-dune-prevention-efficace-des-risques-professionnels/',\n",
       " 'https://www.spotemploi.com/ent-tours-se-connecter-en-ligne/',\n",
       " 'https://www.spotemploi.com/mycampus-connexion-a-mycampus-eduservices/',\n",
       " 'https://www.spotemploi.com/les-cles-pour-reussir-son-test-psychotechnique-sncf/',\n",
       " 'https://www.spotemploi.com/cet-apres-midi-ou-cette-apres-midi-une-guide-sur-lorthographe-correcte/',\n",
       " 'https://www.spotemploi.com/bts-decouvrez-lage-de-chaque-membre-de-ce-groupe-de-k-pop-sensationnel/',\n",
       " 'https://www.spotemploi.com/bts-decouvrez-lage-de-chaque-membre-de-ce-groupe-de-k-pop-sensationnel-2/',\n",
       " 'https://www.spotemploi.com/leo-uga-decouvrez-les-programmes-et-ressources-de-luniversite-de-georgie/',\n",
       " 'https://www.spotemploi.com/optimiser-lutilisation-de-la-messagerie-academique-nancy-metz/',\n",
       " 'https://www.spotemploi.com/ent-u-bordeaux-portail-universitaire/',\n",
       " 'https://www.spotemploi.com/optimisez-votre-experience-dapprentissage-avec-mycampus-eduservices/',\n",
       " 'https://www.spotemploi.com/limpact-de-myschool-escp-sur-lexperience-etudiant/',\n",
       " 'https://www.spotemploi.com/creer-un-dossier-sur-pix-le-guide-exhaustif/',\n",
       " 'https://www.spotemploi.com/metier-par-w-levolution-technologique-dans-le-domaine-du-wagonnage/',\n",
       " 'https://www.spotemploi.com/decouvrir-les-opportunites-de-metis-afpa-fr-pour-une-requalification-professionnelle-reussie/',\n",
       " 'https://www.spotemploi.com/luniversite-dangers-ent-revolution-numerique-au-sein-du-campus/',\n",
       " 'https://www.spotemploi.com/comment-lauthentification-a-poitiers-transforme-lacces-aux-ressources-educatives/',\n",
       " 'https://www.spotemploi.com/escp-blackboard-guide-pour-les-etudiants-et-enseignants/',\n",
       " 'https://www.spotemploi.com/mycampus-eduservices-guide-pour-naviguer-facilement-sur-la-plateforme/',\n",
       " 'https://www.spotemploi.com/prime-informatique-enseignant-2025-nouveautes-et-modalites-de-versement/',\n",
       " 'https://www.spotemploi.com/limportance-cruciale-de-la-palette-euro-pour-une-chaine-dapprovisionnement-efficiente/',\n",
       " 'https://www.spotemploi.com/akeonet-comment-acceder-a-vos-outils-professionnels-facilement/',\n",
       " 'https://www.spotemploi.com/comprendre-le-role-du-pai-dans-leducation-nationale-enjeux-et-perspectives/',\n",
       " 'https://www.spotemploi.com/quel-est-le-metier-le-plus-epanouissant/',\n",
       " 'https://www.spotemploi.com/univo-revolutionne-le-monde-des-plateformes-de-e-learning/',\n",
       " 'https://www.spotemploi.com/comment-rediger-une-lettre-de-motivation-pour-une-demande-de-formation/',\n",
       " 'https://www.spotemploi.com/les-avantages-de-choisir-ornicar-pour-son-permis-de-conduire/',\n",
       " 'https://www.spotemploi.com/les-stylos-4-couleurs-puff-un-outil-essentiel-pour-les-etudiants-et-les-professionnels/',\n",
       " 'https://www.spotemploi.com/formation-permis-poids-lourd-tout-savoir-sur-le-financement-via-le-cpf/',\n",
       " 'https://www.spotemploi.com/comment-se-reconvertir-dans-la-data-science/',\n",
       " 'https://www.spotemploi.com/devenir-kinesitherapeute-etapes-et-conseils-pour-une-reconversion-reussie/',\n",
       " 'https://www.spotemploi.com/rediger-un-chapo-efficace-techniques-et-pieges-a-eviter-en-ecriture/',\n",
       " 'https://www.spotemploi.com/zoom-sur-le-salaire-dun-instituteur-primaire-chiffres-et-perspectives-devolution/',\n",
       " 'https://www.spotemploi.com/methode-nerac-expliquee-avantages-et-fonctionnement-pour-votre-strategie/',\n",
       " 'https://www.spotemploi.com/droit-aux-vacances-en-bts-alternance-ce-quil-faut-savoir/',\n",
       " 'https://www.spotemploi.com/methode-nerac-expliquee-avantages-et-processus-pour-optimiser-votre-strategie/',\n",
       " 'https://www.spotemploi.com/enneagramme-type-2-quest-ce-que-cest/',\n",
       " 'https://www.spotemploi.com/seuil-dimposition-a-partir-de-combien-de-revenus-doit-on-payer-des-impots/',\n",
       " 'https://www.spotemploi.com/optimiser-vos-travaux-grace-au-tableau-de-conversion-litre-en-metre-cube/',\n",
       " 'https://www.spotemploi.com/a-quel-age-on-passe-le-bac-informations-officielles/',\n",
       " 'https://www.spotemploi.com/changer-de-conseiller-pole-emploi-demarches-et-conseils/',\n",
       " 'https://www.spotemploi.com/reflexologie-et-sante-limportance-dune-formation-solide/',\n",
       " 'https://www.spotemploi.com/exploration-des-meilleures-agences-dinterim-a-evreux-votre-chemin-vers-lemploi/',\n",
       " 'https://www.spotemploi.com/les-avantages-dune-formation-en-alternance-en-hotellerie-et-tourisme/',\n",
       " 'https://www.spotemploi.com/integrer-le-developpement-durable-dans-son-cursus-en-ecole-de-commerce/',\n",
       " 'https://www.spotemploi.com/qui-peut-beneficier-dune-formation-par-pole-emploi/',\n",
       " 'https://www.spotemploi.com/comment-sinscrire-a-pole-emploi-sans-attestation-employeur/',\n",
       " 'https://www.spotemploi.com/devenir-digital-nomad/',\n",
       " 'https://www.spotemploi.com/comment-devenir-un-conducteur-livreur-sur-vehicules-utilitaires-legers-niv-3-niveau-cap/',\n",
       " 'https://www.spotemploi.com/permis-de-conduire-en-ligne-les-avantages/',\n",
       " 'https://www.spotemploi.com/comment-amenager-son-temps-de-travail/',\n",
       " 'https://www.spotemploi.com/comprendre-lexperience-employe/',\n",
       " 'https://www.spotemploi.com/le-conge-paternite-explique-vos-droits/',\n",
       " 'https://www.spotemploi.com/quest-ce-que-le-cpf-de-transition-professionnelle/',\n",
       " 'https://www.spotemploi.com/cet-apres-midi-ou-cette-apres-midi-une-guide-sur-lorthographe-correcte/',\n",
       " 'https://www.spotemploi.com/comment-sinscrire-a-pole-emploi-sans-attestation-employeur/',\n",
       " 'https://www.spotemploi.com/mycampus-connexion-a-mycampus-eduservices/']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_articles_on_page(get_all_results_pages(get_max_page_results(top_search_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupérer les infos qu'on souhaite sur chaque article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_info(r, url_article):\n",
    "    # Liste de variables\n",
    "    noms_variables = [\n",
    "        'url',  # URL scrapée\n",
    "        'canonical_url', 'slug', 'meta_title', 'meta_desc',  # Infos issues de la balise meta\n",
    "        'date_published', 'date_modified',  # Infos dates\n",
    "        'author',  # Parfois en meta\n",
    "        'title', 'category', 'views', 'reading_time',  # Metadonnées contenues ailleurs que dans la balise meta\n",
    "        'content', 'raw_content'  # Contenu de la page\n",
    "    ]\n",
    "    \n",
    "    # Initialisation du dictionnaire\n",
    "    data = {nom: np.nan for nom in noms_variables}\n",
    "    \n",
    "    data['url'] = url_article\n",
    "    \n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    \n",
    "    try:\n",
    "        data['canonical_url'] = soup.find('link', {'rel': 'canonical'})['href']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        data['slug'] = data['canonical_url'].split('/')[-2] if data['canonical_url'] else np.nan\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['meta_title'] = soup.find('meta', {'property': 'og:title'})['content']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['meta_desc'] = soup.find('meta', {'property': 'og:description'})['content']\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['date_published'] = pd.to_datetime(soup.find('meta', {'property': 'article:published_time'})['content'][:-6], utc=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['date_modified'] = pd.to_datetime(soup.find('meta', {'property': 'article:modified_time'})['content'][:-6], utc=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['author'] = soup.find(\"meta\", {'name': 'author'})['content']\n",
    "    except:\n",
    "        try:\n",
    "            data['author'] = soup.find(class_=\"author\").text.split('Publié par ')[-1]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        data['title'] = soup.find('h1').text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['category'] = \", \".join([link.text.strip() for link in soup.find(\"div\", class_=\"td-post-header\").find_all(\"a\")])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['views'] = int(soup.find('span', class_=lambda x: x and x.startswith('td-nr-views-')).text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['reading_time'] = int(soup.find('meta', {'name': 'twitter:data2'})['content'].split(' ')[0]) if soup.find('meta', {'name': 'twitter:data2'}) and 'minutes' in soup.find('meta', {'name': 'twitter:data2'})['content'] else np.nan\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['content'] = BeautifulSoup(str(soup.find(\"div\", class_=\"td-post-content\")), 'lxml').get_text()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        data['raw_content'] = str(soup.find(\"div\", class_=\"td-post-content\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Ajout de features\n",
    "    data['length'] = len(str(data['content']).split())\n",
    "    \n",
    "    if pd.notnull(data['date_published']):\n",
    "        data['days_since_published'] = int((pd.Timestamp.now(tz='UTC') - data['date_published']).days)\n",
    "    else:\n",
    "        data['days_since_published'] = np.nan\n",
    "    \n",
    "    # Création de la série\n",
    "    series = pd.Series(data)\n",
    "    series.name = url_article\n",
    "    \n",
    "    # Ajout de features supplémentaires\n",
    "    if pd.notnull(data.get('views')) and pd.notnull(data.get('days_since_published')):\n",
    "        series['views_daily'] = series['views'] / series['days_since_published']\n",
    "        series['views_monthly'] = series['views_daily'] * 30\n",
    "    else:\n",
    "        series['views_daily'] = np.nan\n",
    "        series['views_monthly'] = np.nan\n",
    "    \n",
    "    series['website'] = top_url\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d807fbe4df2d48aaa172d6a3f6900a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "article_list = get_articles_on_page(get_all_results_pages(get_max_page_results(top_search_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175238c6ec384f24858d1477b27d6f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1 / 1:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping terminé. Total d'articles : 57\n",
      "2.9741158485412598\n"
     ]
    }
   ],
   "source": [
    "# Import des packages\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# Initialisation du compteur\n",
    "start_time = time.time()\n",
    "\n",
    "# Fonction principale qui sauvegarde le DataFrame dans un fichier CSV et le retourne\n",
    "def scrape_all_articles(urls, batch_size=250, max_workers=20):\n",
    "    # Initialisation d'une session réutilisable\n",
    "    session = rq.Session()\n",
    "\n",
    "    # Création d'une fonction qui traite chaque URL\n",
    "    def process_url(url):\n",
    "            try:\n",
    "                r = session.get(url)\n",
    "                return get_article_info(r, url)\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur pour {url}: {str(e)}\")\n",
    "                return pd.Series(name=url)\n",
    "\n",
    "    # Utilise ThreadPoolExecutor pour paralléliser le traitement\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Parcourt la liste d'URLs par lots de taille 'batch_size'\n",
    "        for i in range(0, len(urls), batch_size):\n",
    "            # Extrait un lot d'URLs\n",
    "            batch = urls[i:i+batch_size]\n",
    "            # Crée et soumet des tâches pour chaque URL du lot\n",
    "            futures = [executor.submit(process_url, url) for url in batch]\n",
    "            \n",
    "            # Traite les résultats au fur et à mesure qu'ils sont terminés\n",
    "            for future in tqdm(as_completed(futures), total=len(batch), desc=f\"Batch {i//batch_size + 1} / {len(urls)//batch_size+1}\"):\n",
    "                # Récupère le résultat de la tâche\n",
    "                result = future.result()\n",
    "                # Si le résultat n'est pas None (pas d'erreur), l'ajoute aux résultats\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "    \n",
    "    # Création du DataFrame final\n",
    "    df_final = pd.DataFrame(results)\n",
    "    \n",
    "    # Ajout de colonnes calculées\n",
    "    df_final['website'] = top_url\n",
    "    df_final['scraping_date'] = pd.Timestamp.now(tz='UTC')\n",
    "\n",
    "    df_final.columns = ['article_url', 'article_canonical_url', 'article_slug',\n",
    "       'article_meta_title', 'article_meta_desc', 'article_date_published',\n",
    "       'article_date_modified', 'article_author', 'article_title',\n",
    "       'article_category', 'article_views', 'article_reading_time',\n",
    "       'article_content', 'article_raw_content', 'article_length',\n",
    "       'days_since_published', 'article_views_daily', 'article_views_monthly',\n",
    "       'website', 'scraping_date']\n",
    "\n",
    "    # Sauvegarde en CSV\n",
    "    df_final.to_csv(f'scraping_{abbr}.csv', sep='|', index=True)\n",
    "    print(f\"Scraping terminé. Total d'articles : {len(df_final)}\")\n",
    "    return df_final\n",
    "\n",
    "df = scrape_all_articles(article_list)\n",
    "\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27381d7e0174d429bf29c0b2dae57d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\enzol\\anaconda3\\Lib\\site-packages\\ydata_profiling\\model\\correlations.py:66: UserWarning: There was an attempt to calculate the auto correlation, but this failed.\n",
      "To hide this warning, disable the calculation\n",
      "(using `df.profile_report(correlations={\"auto\": {\"calculate\": False}})`\n",
      "If this is problematic for your use case, please report this as an issue:\n",
      "https://github.com/ydataai/ydata-profiling/issues\n",
      "(include the error message: 'cannot reindex on an axis with duplicate labels')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be42539fbda94de895915e0ad4a4a5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0cb1d88f044997814a48ffae69d952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d85e5930647416390b3d378ffabb6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Création d'un rapport de profil avec pandas_profiling\n",
    "profile = ProfileReport(df, title=f\"{abbr.title()} Scraping Report\", explorative=True)\n",
    "\n",
    "# Génération du rapport au format HTML\n",
    "profile.to_file(f\"scraping_report_{abbr}.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
